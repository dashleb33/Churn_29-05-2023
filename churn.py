# -*- coding: utf-8 -*-
"""Churn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18eiv0wc-1BK5MLuMUnw7rQXT9TOHQp9U

#Полезные ссылки:

https://towardsdatascience.com/modeling-customer-lifetime-value-with-lifetimes-71171a35f654 - lifetimes

https://btyd.readthedocs.io/en/latest/User%20Guide.html#rfm-data-format - btyd user guide

https://btyd.readthedocs.io/en/latest/api_reference.html - btyd docs

# Импорт библиотек
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

!pip install lifetimes
!pip install --upgrade btyd
from btyd import BetaGeoFitter
from btyd.utils import summary_data_from_transaction_data, calibration_and_holdout_data
from btyd.plotting import plot_probability_alive_matrix

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_validate, ShuffleSplit, GridSearchCV
from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline

import warnings
warnings.filterwarnings('ignore')

"""#Преобразование данных в RFM формат

## С использованием библиотеки
"""

df = pd.read_parquet('/content/drive/MyDrive/Practice/wallet_urfu.parquet.gzip')
df

rfm = summary_data_from_transaction_data(transactions=df,
                                         customer_id_col='partner',
                                         datetime_col='rep_date',
                                         observation_period_end=('2023-02-23'))
rfm

rfm.to_parquet('/content/drive/MyDrive/Practice/rfm_sum_data.parquet.gzip', compression='gzip')

rfm_cal_holdout = calibration_and_holdout_data(
    transactions=df,
    customer_id_col='partner',
    datetime_col='rep_date',
    calibration_period_end='2022-11-20',
    observation_period_end='2023-02-23'
    )

rfm_cal_holdout

rfm_cal_holdout.to_parquet('/content/drive/MyDrive/Practice/rfm_cal_holdout.parquet.gzip', compression='gzip')

"""## Вручную"""

df

df['rep_date'] = pd.to_datetime(df['rep_date'])

frequency = df['partner'].value_counts()
df_frequency = pd.DataFrame({'partner':frequency.index, 'frequency':frequency.values})
df_frequency.set_index('partner', inplace=True)
df_frequency

first_date = df.groupby('partner')['rep_date'].first().to_frame()
first_date.rename(columns = {'rep_date': 'first_date'}, inplace=True)
first_date

last_date = df.groupby('partner')['rep_date'].last().to_frame()
last_date.rename(columns = {'rep_date': 'last_date'}, inplace=True)
last_date

result_df = first_date.merge(last_date, on = 'partner')
result_df = result_df.merge(df_frequency, on = 'partner')

end_date = '2023-02-23'
result_df['end_date'] = end_date
result_df['end_date'] = pd.to_datetime(result_df['end_date'])

result_df['recency'] = result_df['last_date'] - result_df['first_date']
result_df['T'] = result_df['end_date'] - result_df['first_date']
result_df['recency'] = result_df['recency'].dt.days.astype('int')
result_df['T'] = result_df['T'].dt.days.astype('int')
result_df.drop(columns = ['first_date', 'last_date', 'end_date'], inplace = True)
result_df

result_df.to_parquet('/content/drive/MyDrive/Practice/rfm_without_library.parquet.gzip', compression='gzip')

df_cal = df[df['rep_date'] < '2022-11-20']
df_cal

df_holdout = df[df['rep_date'] > '2022-11-20']
df_holdout

frequency_cal = df_cal['partner'].value_counts()
df_cal_frequency = pd.DataFrame({'partner':frequency_cal.index, 'frequency_cal':frequency_cal.values})
df_cal_frequency.set_index('partner', inplace=True)
df_cal_frequency

frequency_holdout = df_holdout['partner'].value_counts()
df_holdout_frequency = pd.DataFrame({'partner':frequency_holdout.index, 'frequency_holdout':frequency_holdout.values})
df_holdout_frequency.set_index('partner', inplace=True)
df_holdout_frequency

first_date_cal = df_cal.groupby('partner')['rep_date'].first().to_frame()
first_date_cal.rename(columns = {'rep_date': 'first_date_cal'}, inplace=True)
first_date_cal

last_date_cal = df_cal.groupby('partner')['rep_date'].last().to_frame()
last_date_cal.rename(columns = {'rep_date': 'last_date_cal'}, inplace=True)
last_date_cal

result_df_cal = first_date_cal.merge(last_date_cal, on = 'partner')
result_df_cal = result_df_cal.merge(df_cal_frequency, on = 'partner')

end_cal_date = '2022-11-20'
result_df_cal['end_cal_date'] = end_cal_date
result_df_cal['end_cal_date'] = pd.to_datetime(result_df_cal['end_cal_date'])

result_df_cal['recency_cal'] = result_df_cal['last_date_cal'] - result_df_cal['first_date_cal']
result_df_cal['T_cal'] = result_df_cal['end_cal_date'] - result_df_cal['first_date_cal']
result_df_cal['recency_cal'] = result_df_cal['recency_cal'].dt.days.astype('int')
result_df_cal['T_cal'] = result_df_cal['T_cal'].dt.days.astype('int')
result_df_cal.drop(columns = ['first_date_cal', 'last_date_cal', 'end_cal_date'], inplace = True)
result_df_cal

result_df_cal_holdout = result_df_cal.merge(df_holdout_frequency, how ='left', on = 'partner')
result_df_cal_holdout['frequency_holdout'].fillna(0, inplace = True)

end_cal_date = pd.to_datetime('2022-11-20')
end_holdout_date = pd.to_datetime('2023-02-23')
result_df_cal_holdout['duration_holdout'] = end_holdout_date - end_cal_date
result_df_cal_holdout['duration_holdout'] = result_df_cal_holdout['duration_holdout'].dt.days.astype('int')
result_df_cal_holdout

result_df_cal_holdout.to_parquet('/content/drive/MyDrive/Practice/rfm_cal_holdout_without_library.parquet.gzip', compression='gzip')

"""#bgf, LogReg

## С использованием библиотеки
"""

data = pd.read_parquet('/content/drive/MyDrive/Practice/rfm_sum_data.parquet.gzip')
data

data_cal_holdout = pd.read_parquet('/content/drive/MyDrive/Practice/rfm_cal_holdout.parquet.gzip')
data_cal_holdout

data_clear = data_cal_holdout[data_cal_holdout['T_cal'] - data_cal_holdout['recency_cal'] < 95]
data_clear

data_full = pd.merge(data_clear, data, on = 'partner')
data_full

data_full['in_d_c'] = data_full['T_cal'] - data_full['recency_cal']
data_full['in_d_f'] = round((data_full['recency'] - data_full['T_cal'])/ data_full['frequency_holdout']) + data_full['in_d_c']
data_full.loc[data_full['in_d_f'] < 0, 'in_d_f'] = 100
data_full['in_d_f'].fillna(100, inplace = True)
data_full

bgf = BetaGeoFitter(penalizer_coef=0.0001)

bgf.fit(frequency=data_full['frequency_cal'], 
        recency=data_full['recency_cal'], 
        T=data_full['T_cal'])

plt.figure(figsize = (10, 10))
plot_probability_alive_matrix(bgf)

alive_prob = bgf.conditional_probability_alive(frequency=data_full['frequency_cal'], 
                                               recency=data_full['recency_cal'], 
                                               T=data_full['T_cal'])
data_full['alive_prob'] = alive_prob
data_full

t = 95 - data_full['in_d_c']

expected_purchases = bgf.predict(t, frequency=data_full['frequency_cal'], 
                                  recency=data_full['recency_cal'], 
                                  T=data_full['T_cal'])

data_full['expected_purchases'] = expected_purchases
data_full

data_full['avarage_purchases'] = data_full['frequency_cal'] / data_full['recency_cal']
data_full['avarage_purchases'].fillna(0, inplace = True)
data_full

data_full['Churn'] = np.where(data_full['in_d_f'] >= 95, True, False)
data_full.drop(['duration_holdout', 'frequency_holdout','frequency', 'recency', 'T', 'in_d_f'], axis = 1).corr().style.background_gradient(cmap="vlag", vmin = -1, vmax=1)

data_full[data_full['Churn'] == True]['Churn'].count()

train, test = train_test_split(data_full, test_size=0.3, random_state=42)
x_train = train[['frequency_cal','recency_cal','T_cal','in_d_c', 'alive_prob', 'expected_purchases', 'avarage_purchases']]
y_train = train['Churn']
x_test = test[['frequency_cal','recency_cal','T_cal','in_d_c', 'alive_prob', 'expected_purchases', 'avarage_purchases']]
y_test = test['Churn']

result = pd.DataFrame(columns = ['Classifier', 'Precision', 'Recall', 'F1'])

LogReg = LogisticRegression(random_state=42).fit(x_train, y_train)
y_predict = LogReg.predict(x_test)
result = pd.DataFrame(np.array([['LogReg', round(precision_score(y_test, y_predict), 3), round(recall_score(y_test, y_predict), 3),
                                 round(f1_score(y_test, y_predict), 3)]]), columns=['Classifier', 'Precision', 'Recall', 'F1']).append(result, ignore_index=True)

DTC = DecisionTreeClassifier(max_depth = 5, random_state=42).fit(x_train, y_train)
y_predict = DTC.predict(x_test)
result = pd.DataFrame(np.array([['DTC', round(precision_score(y_test, y_predict), 3), round(recall_score(y_test, y_predict), 3),
                                 round(f1_score(y_test, y_predict), 3)]]), columns=['Classifier', 'Precision', 'Recall', 'F1']).append(result, ignore_index=True)

RFC = RandomForestClassifier(n_estimators=200, max_depth=3, random_state = 42, n_jobs=-1).fit(x_train, y_train)
y_predict = RFC.predict(x_test)
result = pd.DataFrame(np.array([['RFC', round(precision_score(y_test, y_predict), 3), round(recall_score(y_test, y_predict), 3),
                                 round(f1_score(y_test, y_predict), 3)]]), columns=['Classifier', 'Precision', 'Recall', 'F1']).append(result, ignore_index=True)

GBC = GradientBoostingClassifier(n_estimators = 100, learning_rate = 1.0, max_depth = 2, random_state = 42).fit(x_train, y_train)
y_predict = GBC.predict(x_test)
result = pd.DataFrame(np.array([['GBC', round(precision_score(y_test, y_predict), 3), round(recall_score(y_test, y_predict), 3),
                                 round(f1_score(y_test, y_predict), 3)]]), columns=['Classifier', 'Precision', 'Recall', 'F1']).append(result, ignore_index=True)

BC = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, random_state = 42).fit(x_train, y_train)
y_predict = BC.predict(x_test)
result = pd.DataFrame(np.array([['BC', round(precision_score(y_test, y_predict), 3), round(recall_score(y_test, y_predict), 3),
                                 round(f1_score(y_test, y_predict), 3)]]), columns=['Classifier', 'Precision', 'Recall', 'F1']).append(result, ignore_index=True)

result.sort_values('F1', ascending=False)

scoring = {'f1': 'f1',
           'precision': 'precision',
           'recall': 'recall'}

scores = cross_validate(DTC, x_test, y_test, scoring=scoring, cv=ShuffleSplit(n_splits=5, random_state=42))
DF_cv = pd.DataFrame(scores)
print('Результаты Кросс-валидации \n', DF_cv, '\n')
print(round(DF_cv.mean()[2:], 2))

y_predict = DTC.predict(x_test)
cm = confusion_matrix(y_test, y_predict)
cm_display = ConfusionMatrixDisplay(cm, display_labels=['Non-churned', 'Churned']).plot()

"""## Вручную"""

data = pd.read_parquet('/content/drive/MyDrive/Practice/rfm_without_library.parquet.gzip')
data

data_cal_holdout = pd.read_parquet('/content/drive/MyDrive/Practice/rfm_cal_holdout_without_library.parquet.gzip')
data_cal_holdout

data_clear = data_cal_holdout[data_cal_holdout['T_cal'] - data_cal_holdout['recency_cal'] < 95]
data_clear

data_full = pd.merge(data_clear, data, on = 'partner')
data_full

data_full['in_d_c'] = data_full['T_cal'] - data_full['recency_cal']
data_full['in_d_f'] = round((data_full['recency'] - data_full['T_cal'])/ data_full['frequency_holdout']) + data_full['in_d_c']
data_full.loc[data_full['in_d_f'] < 0, 'in_d_f'] = 100
data_full['in_d_f'].fillna(100, inplace = True)
data_full

bgf = BetaGeoFitter(penalizer_coef=0.0001)

bgf.fit(frequency=data_full['frequency_cal'], 
        recency=data_full['recency_cal'], 
        T=data_full['T_cal'])

plt.figure(figsize = (10, 10))
plot_probability_alive_matrix(bgf)

alive_prob = bgf.conditional_probability_alive(frequency=data_full['frequency_cal'], 
                                               recency=data_full['recency_cal'], 
                                               T=data_full['T_cal'])
data_full['alive_prob'] = alive_prob
data_full

t = 95 - data_full['in_d_c']

expected_purchases = bgf.predict(t, frequency=data_full['frequency_cal'], 
                                  recency=data_full['recency_cal'], 
                                  T=data_full['T_cal'])

data_full['expected_purchases'] = expected_purchases
data_full

data_full['avarage_purchases'] = data_full['frequency_cal'] / data_full['recency_cal']
data_full['avarage_purchases'].fillna(0, inplace = True)
data_full.loc[data_full['avarage_purchases'] == np.inf, 'avarage_purchases'] = data_full['frequency_cal']
data_full

data_full['Churn'] = np.where(data_full['in_d_f'] >= 95, True, False)
data_full.drop(['duration_holdout', 'frequency_holdout','frequency', 'recency', 'T', 'in_d_f'], axis = 1).corr().style.background_gradient(cmap="vlag", vmin = -1, vmax=1)

data_full[data_full['Churn'] == True]['Churn'].count()

train, test = train_test_split(data_full, test_size=0.3, random_state=42)
x_train = train[['frequency_cal','recency_cal','T_cal','in_d_c', 'alive_prob', 'expected_purchases', 'avarage_purchases']]
y_train = train['Churn']
x_test = test[['frequency_cal','recency_cal','T_cal','in_d_c', 'alive_prob', 'expected_purchases', 'avarage_purchases']]
y_test = test['Churn']

result1 = pd.DataFrame(columns = ['Classifier', 'Precision', 'Recall', 'F1'])

LogReg = LogisticRegression(random_state=42).fit(x_train, y_train)
y_predict = LogReg.predict(x_test)
result1 = pd.DataFrame(np.array([['LogReg', round(precision_score(y_test, y_predict), 3), round(recall_score(y_test, y_predict), 3),
                                 round(f1_score(y_test, y_predict), 3)]]), columns=['Classifier', 'Precision', 'Recall', 'F1']).append(result1, ignore_index=True)

DTC = DecisionTreeClassifier(max_depth = 5, random_state=42).fit(x_train, y_train)
y_predict = DTC.predict(x_test)
result1 = pd.DataFrame(np.array([['DTC', round(precision_score(y_test, y_predict), 3), round(recall_score(y_test, y_predict), 3),
                                 round(f1_score(y_test, y_predict), 3)]]), columns=['Classifier', 'Precision', 'Recall', 'F1']).append(result1, ignore_index=True)

RFC = RandomForestClassifier(n_estimators=200, max_depth=3, random_state = 42, n_jobs=-1).fit(x_train, y_train)
y_predict = RFC.predict(x_test)
result1 = pd.DataFrame(np.array([['RFC', round(precision_score(y_test, y_predict), 3), round(recall_score(y_test, y_predict), 3),
                                 round(f1_score(y_test, y_predict), 3)]]), columns=['Classifier', 'Precision', 'Recall', 'F1']).append(result1, ignore_index=True)

GBC = GradientBoostingClassifier(n_estimators = 100, learning_rate = 1.0, max_depth = 2, random_state = 42).fit(x_train, y_train)
y_predict = GBC.predict(x_test)
result1 = pd.DataFrame(np.array([['GBC', round(precision_score(y_test, y_predict), 3), round(recall_score(y_test, y_predict), 3),
                                 round(f1_score(y_test, y_predict), 3)]]), columns=['Classifier', 'Precision', 'Recall', 'F1']).append(result1, ignore_index=True)

BC = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, random_state = 42).fit(x_train, y_train)
y_predict = BC.predict(x_test)
result1 = pd.DataFrame(np.array([['BC', round(precision_score(y_test, y_predict), 3), round(recall_score(y_test, y_predict), 3),
                                 round(f1_score(y_test, y_predict), 3)]]), columns=['Classifier', 'Precision', 'Recall', 'F1']).append(result1, ignore_index=True)

result1.sort_values('F1', ascending=False)

scoring = {'f1': 'f1',
           'precision': 'precision',
           'recall': 'recall'}

scores = cross_validate(DTC, x_test, y_test, scoring=scoring, cv=ShuffleSplit(n_splits=5, random_state=42))
DF_cv = pd.DataFrame(scores)
print('Результаты Кросс-валидации \n', DF_cv, '\n')
print(round(DF_cv.mean()[2:], 2))

y_predict = DTC.predict(x_test)
cm = confusion_matrix(y_test, y_predict)
cm_display = ConfusionMatrixDisplay(cm, display_labels=['Non-churned', 'Churned']).plot()

with open('/content/drive/MyDrive/Practice/best_model.pkl', 'wb') as file:
    pickle.dump(DTC, file)

with open('/content/drive/MyDrive/Practice/best_model.pkl', 'rb') as file:
    pickle_model = pickle.load(file)

pickle_model.predict(x_test)

print('Результаты на тестовых данных', '\n f1:', round(f1_score(y_test, y_predict), 3), 'precision:',
      round(precision_score(y_test, y_predict), 3), 'recall:', round(recall_score(y_test, y_predict), 3))